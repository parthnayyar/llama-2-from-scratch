{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fcd45b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "from parser import GrammarConstrainedParser\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7fde41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTokenizer(SentencePieceProcessor):\n",
    "    def __init__(self):\n",
    "        self._tokens = [\n",
    "        # Structural tokens\n",
    "        '{',\n",
    "        '}',\n",
    "        ':',\n",
    "        ',',\n",
    "        '\"',\n",
    "        # Field names\n",
    "        '\"name\"',\n",
    "        '\"age\"',\n",
    "        '\"gender\"',\n",
    "        '\"employed\"',\n",
    "        # Values\n",
    "        '\"John\"',\n",
    "        '30',\n",
    "        '\"Male\"',\n",
    "        '\"Female\"',\n",
    "        'true',\n",
    "        'false',\n",
    "        # Partial tokens\n",
    "        '{\"',\n",
    "        'nam',\n",
    "        'e\":',\n",
    "        ' \"',\n",
    "        'Jo',\n",
    "        'hn\"',\n",
    "        ', \"',\n",
    "        'ag',\n",
    "        'e\": ',\n",
    "        '3',\n",
    "        '0, ',\n",
    "        '\"gen',\n",
    "        'der',\n",
    "        '\": \"',\n",
    "        'Ma',\n",
    "        'le\"',\n",
    "        ', \"e',\n",
    "        'mpl',\n",
    "        'oyed',\n",
    "        '\": ',\n",
    "        'tr',\n",
    "        'ue',\n",
    "        # Multi-token combinations\n",
    "        '{\"name\": \"',\n",
    "        '{\"name\": \"John\", ',\n",
    "        '\"age\": 30',\n",
    "        ', \"gender\": \"Male\"',\n",
    "        ', \"employed\": true',\n",
    "        '}'\n",
    "    ]\n",
    "    def vocab_size(self):\n",
    "        return len(self._tokens)\n",
    "    def Decode(self, i):\n",
    "        return self._tokens[i]\n",
    "    def eos_id(self):\n",
    "        return -1\n",
    "tokenizer = TestTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6b9d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(BaseModel): \n",
    "    name: str\n",
    "    age: int\n",
    "    gender: Literal[\"Male\", \"Female\"]\n",
    "    employed: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b583fa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "STR: /\"[^\"]*\"/\n",
    "INT: /-?\\d+/\n",
    "FLOAT: /-?\\d+\\.\\d*/\n",
    "BOOL: \"true\" | \"false\"\n",
    "NONE: \"null\"\n",
    "\n",
    "LBRACE: \"{\"\n",
    "RBRACE: \"}\"\n",
    "LSQB: \"[\"\n",
    "RSQB: \"]\"\n",
    "LPAREN: \"(\"\n",
    "RPAREN: \")\"\n",
    "COLON: \":\"\n",
    "COMMA: \",\"\n",
    "\n",
    "start: person\n",
    "\n",
    "person: LBRACE person_field_name COMMA person_field_age COMMA person_field_gender COMMA person_field_employed RBRACE\n",
    "\n",
    "person_field_name: NAME COLON STR\n",
    "person_field_age: AGE COLON INT\n",
    "person_field_gender: GENDER COLON LITERAL_GENDER\n",
    "person_field_employed: EMPLOYED COLON BOOL\n",
    "\n",
    "NAME.1: \"\\\"name\\\"\"\n",
    "AGE.1: \"\\\"age\\\"\"\n",
    "GENDER.1: \"\\\"gender\\\"\"\n",
    "EMPLOYED.1: \"\\\"employed\\\"\"\n",
    "LITERAL_GENDER.1: \"\\\"Male\\\"\" | \"\\\"Female\\\"\"\n",
    "\n",
    "%ignore /\\s+/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "525e8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser: GrammarConstrainedParser = GrammarConstrainedParser(grammar=grammar, llm_tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69881c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed till now: \n",
      "Acceptable token ids: [0, 37, 38, 15]\n",
      "Acceptable tokens: ['{', '{\"name\": \"', '{\"name\": \"John\", ', '{\"']\n",
      "Randomly accepting token id: 0 ('{')\n",
      "\n",
      "Parsed till now: {\n",
      "Acceptable token ids: [18, 4, 5]\n",
      "Acceptable tokens: [' \"', '\"', '\"name\"']\n",
      "Randomly accepting token id: 18 (' \"')\n",
      "\n",
      "Parsed till now: { \"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Acceptable token ids: [16]\n",
      "Acceptable tokens: ['nam']\n",
      "Randomly accepting token id: 16 ('nam')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(3):\n",
    "    print(\"Parsed till now:\", parser.parsed_str)\n",
    "    print(\"Acceptable token ids:\", (ids := parser.get_acceptable_llm_tokens().tolist()))\n",
    "    print(\"Acceptable tokens:\", [tokenizer.Decode(i) for i in ids])\n",
    "    print(\"Randomly accepting token id:\", (id := random.choice(ids)), f\"('{tokenizer.Decode(id)}')\")\n",
    "    parser.accept_token(id)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a472ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.get_acceptable_llm_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4706123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.Decode(0), tokenizer.Decode(15), tokenizer.Decode(37), tokenizer.Decode(38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eca7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.accept_token(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d5ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec91bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5c1f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lark import Lark\n",
    "parser: Lark = Lark(grammar, parser=\"lalr\", lexer=\"basic\")\n",
    "interactive = parser.parse_interactive(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(parser.lex('{\"name\": \"John\", \"age\": 30, \"gender\": -1, \"employed\": '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee63eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a722be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokens:\n",
    "    interactive.feed_token(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450dcee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive.accepts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16456983",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(parser.lex(' true'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee504e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(parser.lex('{  '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a471be",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive.feed_token(Token('LBRACE', '{'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive.accepts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SentencePieceProcessor()\n",
    "tokenizer.Load(\"../Llama-2-7b/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca11c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e28f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "tensor(list(set([1, 2, 3])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-2-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
