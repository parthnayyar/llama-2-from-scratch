{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19231a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from traceback import print_exc\n",
    "from inference import LLaMA\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abcf1f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from Llama-2-7b\\consolidated.00.pth\n",
      "Loaded checkpoint in 64.50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\parth\\anaconda3\\envs\\llama-2-from-scratch\\Lib\\site-packages\\torch\\__init__.py:1240: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:436.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model in 233.16s\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = LLaMA.build_model(\n",
    "    checkpoints_dir=\"Llama-2-7b/\",\n",
    "    tokenizer_path=\"Llama-2-7b/tokenizer.model\",\n",
    "    load_model=True,\n",
    "    max_seq_len=1024,\n",
    "    max_batch_size=4,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0085788b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.model.layers[0].attention.cache_k == 0).all(dim=-1).all(dim=-1).all(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af1f9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 4\n",
    "prompts = [\n",
    "        \"Roses are red, violets are\",\n",
    "        \"7 + 5 =\",\n",
    "        \"\"\"Complete the blank:\n",
    "\n",
    "Q: The capital of France is ____\n",
    "A: Paris\n",
    "\n",
    "Q: The capital of Germany is ____\n",
    "A: Berlin\n",
    "\n",
    "Q: The capital of Italy is ____\n",
    "A: Rome\n",
    "\n",
    "Q: The capital of Spain is ____\n",
    "A:\"\"\"\n",
    "    ]\n",
    "\n",
    "prompts = [\n",
    "        \"Roses are red, violets are\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d69b8aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_p sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tokens: 100%|██████████| 12/12 [03:19<00:00, 16.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roses are red, violets are blue, take one\n",
      "====================================================================================================\n",
      "greedy sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tokens: 100%|██████████| 12/12 [03:17<00:00, 16.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roses are red, violets are blue, I'\n",
      "====================================================================================================\n",
      "random sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tokens: 100%|██████████| 12/12 [03:15<00:00, 16.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roses are red, violets are blue, come be\n",
      "====================================================================================================\n",
      "top_k sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tokens: 100%|██████████| 12/12 [03:15<00:00, 16.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roses are red, violets are ded Ad amgra\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "strategy = \"top_p\"\n",
    "try:\n",
    "    print(f\"{strategy} sampling\")\n",
    "    out_tokens, out_texts = model.generate(prompts, max_gen_len=max_new_tokens, strategy=strategy, p=0.9)\n",
    "    assert len(out_texts) == len(prompts), f\"Expected {len(prompts)} outputs, got {len(out_tokens)}\"\n",
    "    print(f\"\\n{'-'*50}\\n\".join(out_texts))\n",
    "    print(\"=\"*100)\n",
    "except Exception as e:\n",
    "    print(f\"Error during {strategy} sampling: {e}\")\n",
    "    print_exc()\n",
    "\n",
    "strategy = \"greedy\"\n",
    "try:\n",
    "    print(f\"{strategy} sampling\")\n",
    "    out_tokens, out_texts = model.generate(prompts, max_gen_len=max_new_tokens, strategy=strategy)\n",
    "    assert len(out_texts) == len(prompts), f\"Expected {len(prompts)} outputs, got {len(out_tokens)}\"\n",
    "    print(f\"\\n{'-'*50}\\n\".join(out_texts))\n",
    "    print(\"=\"*100)\n",
    "except Exception as e:\n",
    "    print(f\"Error during {strategy} sampling: {e}\")\n",
    "    print_exc()\n",
    "\n",
    "strategy = \"random\"\n",
    "try:\n",
    "    print(f\"{strategy} sampling\")\n",
    "    out_tokens, out_texts = model.generate(prompts, max_gen_len=max_new_tokens, strategy=strategy)\n",
    "    assert len(out_texts) == len(prompts), f\"Expected {len(prompts)} outputs, got {len(out_tokens)}\"\n",
    "    print(f\"\\n{'-'*50}\\n\".join(out_texts))\n",
    "    print(\"=\"*100)\n",
    "except Exception as e:\n",
    "    print(f\"Error during {strategy} sampling: {e}\")\n",
    "    print_exc()\n",
    "\n",
    "strategy = \"top_k\"\n",
    "try:\n",
    "    print(f\"{strategy} sampling\")\n",
    "    out_tokens, out_texts = model.generate(prompts, max_gen_len=max_new_tokens, strategy=strategy, k=50)\n",
    "    assert len(out_texts) == len(prompts), f\"Expected {len(prompts)} outputs, got {len(out_tokens)}\"\n",
    "    print(f\"\\n{'-'*50}\\n\".join(out_texts))\n",
    "    print(\"=\"*100)\n",
    "except Exception as e:\n",
    "    print(f\"Error during {strategy} sampling: {e}\")\n",
    "    print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b34bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _beam1(\n",
    "    model,\n",
    "    tokens: torch.Tensor,                 # (B, L_total)\n",
    "    prompt_tokens_mask: torch.Tensor,     # (B, L_total)  – True on prompt positions\n",
    "    temperature: float,\n",
    "    k: int,\n",
    ") -> tuple[list[list[int]], list[str]]:\n",
    "\n",
    "    device          = tokens.device\n",
    "    B, L_total      = tokens.shape\n",
    "    vocab_size      = model.tokenizer.vocab_size()\n",
    "    eos_id          = model.tokenizer.eos_id\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Duplicate everything k times ------------------------------------------------\n",
    "    # ------------------------------------------------------------------\n",
    "    tokens          = tokens.unsqueeze(1).repeat(1, k, 1)                 # (B, k, L)\n",
    "    prompt_mask     = prompt_tokens_mask.unsqueeze(1).repeat(1, k, 1)     # (B, k, L)\n",
    "\n",
    "    tokens_flat     = tokens.view(B * k, L_total)                         # (B·k, L)\n",
    "    prompt_flat     = prompt_mask.view(B * k, L_total)\n",
    "\n",
    "    # running log‑probs for every beam (B, k)\n",
    "    beam_scores     = torch.zeros(B, k, device=device)                    # log‑probs\n",
    "    finished        = torch.zeros(B, k, dtype=torch.bool, device=device)  # True if <eos> seen\n",
    "\n",
    "    for cur_pos in range(1, L_total):\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 1. Forward last token for every live beam --------------------------\n",
    "        # ------------------------------------------------------------------\n",
    "        with torch.no_grad():\n",
    "            logits = model.model(tokens_flat[:, cur_pos-1:cur_pos], cur_pos)  # (B·k, 1, V)\n",
    "\n",
    "        log_probs = torch.log_softmax(logits[:, -1] / temperature, dim=-1)    # (B·k, V)\n",
    "        log_probs = log_probs.view(B, k, vocab_size)                          # (B, k, V)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 2. Handle (a) prompt positions, (b) finished beams -----------------\n",
    "        # ------------------------------------------------------------------\n",
    "        # a) Padded prompt positions → force given token (prob=0 → log=0)\n",
    "        forced_mask         = prompt_mask[:, :, cur_pos]                      # (B, k)\n",
    "        forced_token_ids    = tokens[:, :, cur_pos]                           # (B, k)\n",
    "\n",
    "        log_probs[forced_mask]  = float(\"-inf\")\n",
    "        log_probs[forced_mask, torch.arange(vocab_size, device=device)]  # type: ignore\n",
    "        log_probs[forced_mask, forced_token_ids] = 0.0                       # keep only the true token\n",
    "\n",
    "        # b) Finished beams → ONLY allow <eos>\n",
    "        log_probs[finished] = float(\"-inf\")\n",
    "        log_probs[finished, eos_id] = 0.0\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 3. Expand & select top‑k per original prompt -----------------------\n",
    "        # ------------------------------------------------------------------\n",
    "        # current beam_scores (B, k)  +  log_probs (B, k, V)  →  (B, k, V)\n",
    "        candidate_scores = beam_scores.unsqueeze(-1) + log_probs            # (B, k, V)\n",
    "        candidate_scores = candidate_scores.view(B, k * vocab_size)         # (B, k·V)\n",
    "\n",
    "        topk_scores, topk_indices = candidate_scores.topk(k, dim=-1)        # (B, k)\n",
    "\n",
    "        topk_beam_idx  = topk_indices // vocab_size                         # (B, k)\n",
    "        topk_token_idx = topk_indices %  vocab_size                         # (B, k)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 4. Gather tokens from the parent beams and write the new token -----\n",
    "        # ------------------------------------------------------------------\n",
    "        batch_idx = torch.arange(B, device=device).unsqueeze(1)             # (B, 1)\n",
    "\n",
    "        # gather the chosen parent beams\n",
    "        tokens = tokens[batch_idx, topk_beam_idx]                           # (B, k, L)\n",
    "        prompt_mask = prompt_mask[batch_idx, topk_beam_idx]                 # (B, k, L)\n",
    "        finished = finished[batch_idx, topk_beam_idx]                       # (B, k)\n",
    "\n",
    "        # write the new token\n",
    "        tokens[:, :, cur_pos] = topk_token_idx\n",
    "        prompt_mask[:, :, cur_pos] = False\n",
    "\n",
    "        # update score & finished flags\n",
    "        beam_scores = topk_scores\n",
    "        finished |= (topk_token_idx == eos_id)\n",
    "\n",
    "        # flatten again for next forward pass\n",
    "        tokens_flat = tokens.view(B * k, L_total)\n",
    "        prompt_flat = prompt_mask.view(B * k, L_total)\n",
    "\n",
    "        # early stop if every prompt has at least one finished beam\n",
    "        if finished.all():  \n",
    "            break\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 5. Pick the best beam per prompt -------------------------------------\n",
    "    # ----------------------------------------------------------------------\n",
    "    best_scores, best_idx = beam_scores.max(dim=-1)                         # (B,)\n",
    "    best_tokens = tokens[batch_idx.squeeze(), best_idx]                     # (B, L)\n",
    "\n",
    "    out_tokens: list[list[int]] = []\n",
    "    out_text:   list[str]       = []\n",
    "\n",
    "    for seq in best_tokens:                                                 # seq: (L,)\n",
    "        seq = seq.tolist()\n",
    "        if eos_id in seq:\n",
    "            seq = seq[:seq.index(eos_id)]\n",
    "        out_tokens.append(seq)\n",
    "        out_text.append(model.tokenizer.decode(seq))\n",
    "\n",
    "    return out_tokens, out_text\n",
    "\n",
    "def _beam2(model, tokens: torch.Tensor, prompt_tokens_mask: torch.Tensor, temperature: float, k: int) -> tuple[list[list[int]], list[str]]:\n",
    "    device = model.model_args.device\n",
    "    batch_size, total_len = tokens.shape\n",
    "    vocab_size = model.model_args.vocab_size\n",
    "\n",
    "    # Expand the tokens and masks to k beams per batch element\n",
    "    expanded_tokens = tokens.repeat_interleave(k, dim=0)  # (batch_size * k, total_len)\n",
    "    expanded_prompt_mask = prompt_tokens_mask.repeat_interleave(k, dim=0)  # (batch_size * k, total_len)\n",
    "\n",
    "    # Initialize beam scores: (batch_size, k)\n",
    "    beam_scores = torch.zeros((batch_size, k), device=device)\n",
    "    beam_scores[:, 1:] = -float('inf')  # Only the first beam is active initially\n",
    "\n",
    "    # Track whether each beam has reached EOS\n",
    "    eos_reached = torch.zeros((batch_size, k), dtype=torch.bool, device=device)\n",
    "\n",
    "    for cur_pos in tqdm(range(1, total_len), desc=\"Generating tokens (beam search)\"):\n",
    "        # Get the logits for the next token\n",
    "        with torch.no_grad():\n",
    "            # Input is the tokens up to cur_pos-1\n",
    "            logits = model.model(expanded_tokens[:, cur_pos-1:cur_pos], cur_pos)\n",
    "\n",
    "        # Apply temperature and get log probabilities\n",
    "        logits = logits[:, -1] / temperature\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)  # (batch_size * k, vocab_size)\n",
    "\n",
    "        # For each beam, select top k tokens and their log probs\n",
    "        topk_log_probs, topk_indices = log_probs.topk(k, dim=-1)  # (batch_size * k, k)\n",
    "\n",
    "        # Reshape to (batch_size, k, k)\n",
    "        topk_log_probs = topk_log_probs.view(batch_size, k, k)\n",
    "\n",
    "        # Compute new scores: beam_scores (batch, k, 1) + topk_log_probs (batch, k, k) => (batch, k, k)\n",
    "        new_scores = beam_scores.unsqueeze(-1) + topk_log_probs\n",
    "\n",
    "        # Flatten to (batch_size, k * k)\n",
    "        new_scores_flat = new_scores.view(batch_size, -1)\n",
    "\n",
    "        # Select top k candidates for each batch element\n",
    "        topk_new_scores, topk_indices_flat = new_scores_flat.topk(k, dim=-1)  # (batch_size, k)\n",
    "\n",
    "        # Determine parent beams and token ranks\n",
    "        parent_beams = topk_indices_flat // k  # (batch_size, k)\n",
    "        token_ranks = topk_indices_flat % k    # (batch_size, k)\n",
    "\n",
    "        # Update beam scores\n",
    "        beam_scores = topk_new_scores\n",
    "\n",
    "        # Gather the token indices for the selected candidates\n",
    "        beam_indices = (torch.arange(batch_size, device=device).unsqueeze(1) * k + parent_beams).view(-1)\n",
    "        token_ranks = token_ranks.view(-1)\n",
    "\n",
    "        # Gather the token IDs from topk_indices\n",
    "        gathered_token_ids = topk_indices[beam_indices, token_ranks]\n",
    "\n",
    "        # Apply prompt mask: if current position is part of the prompt, use the existing token\n",
    "        current_prompt_mask = expanded_prompt_mask[:, cur_pos]\n",
    "        existing_tokens = expanded_tokens[:, cur_pos]\n",
    "        gathered_token_ids = torch.where(current_prompt_mask, existing_tokens, gathered_token_ids)\n",
    "\n",
    "        # Update the expanded_tokens by copying the parent sequences and adding the new token\n",
    "        expanded_tokens[:, :cur_pos] = expanded_tokens[beam_indices, :cur_pos]\n",
    "        expanded_tokens[:, cur_pos] = gathered_token_ids\n",
    "\n",
    "        # Check for EOS tokens in non-prompt positions\n",
    "        eos_reached_new = (~current_prompt_mask) & (gathered_token_ids == model.tokenizer.eos_id)\n",
    "        eos_reached_new = eos_reached_new.view(batch_size, k)\n",
    "\n",
    "        # Update eos_reached and mask scores for completed beams\n",
    "        eos_reached = eos_reached | eos_reached_new\n",
    "        beam_scores[eos_reached] = -float('inf')\n",
    "\n",
    "        # Early stopping if all beams are EOS\n",
    "        if eos_reached.all():\n",
    "            break\n",
    "\n",
    "    # Select the best beam for each batch element\n",
    "    best_beam_indices = beam_scores.argmax(dim=-1)  # (batch_size)\n",
    "    best_beam_indices_expanded = (torch.arange(batch_size, device=device) * k) + best_beam_indices\n",
    "    best_tokens = expanded_tokens[best_beam_indices_expanded]\n",
    "\n",
    "    # Process the final tokens to remove padding and cut at EOS\n",
    "    out_tokens = []\n",
    "    out_text = []\n",
    "    for i in range(batch_size):\n",
    "        tokens_list = best_tokens[i].tolist()\n",
    "        if model.tokenizer.eos_id in tokens_list:\n",
    "            eos_idx = tokens_list.index(model.tokenizer.eos_id)\n",
    "            tokens_list = tokens_list[:eos_idx]\n",
    "        out_tokens.append(tokens_list)\n",
    "        out_text.append(model.tokenizer.decode(tokens_list))\n",
    "\n",
    "    return out_tokens, out_text\n",
    "\n",
    "def generate_with_beam(model, prompts: list[str], temperature: float = 1.0, max_gen_len = None, k: int = 2, func: int = 1) -> tuple[list[list[int]], list[str]]:\n",
    "    assert temperature > 0, f\"Temperature must be greater than 0, got {temperature}\"\n",
    "    if max_gen_len is None:\n",
    "        max_gen_len = model.model_args.max_seq_len - 1\n",
    "    # Convert each prompt into tokens\n",
    "    prompt_tokens = [model.tokenizer.encode(prompt, out_type=int, add_bos=True, add_eos=False) for prompt in prompts]\n",
    "    # Make sure the batch size is not too large\n",
    "    batch_size = len(prompt_tokens)\n",
    "    assert batch_size <= model.model_args.max_batch_size, f\"batch size must be less than or equal to {model.model_args.max_batch_size}\"\n",
    "    max_prompt_len = max(len(prompt) for prompt in prompt_tokens)\n",
    "    # Make sure the prompt length is not larger than the maximum sequence length\n",
    "    assert max_prompt_len <= model.model_args.max_seq_len, f\"prompt length must be less than or equal to {model.model_args.max_seq_len}\"\n",
    "    total_len = min(model.model_args.max_seq_len, max_gen_len + max_prompt_len)\n",
    "\n",
    "    # Create the list that will contain the generated tokens, along with the initial prompt tokens\n",
    "    pad_id = model.tokenizer.pad_id()\n",
    "    tokens = torch.full((batch_size, total_len), pad_id, dtype=torch.long, device=model.model_args.device)\n",
    "    for k, t in enumerate(prompt_tokens):\n",
    "        # Populate the initial tokens with the prompt tokens\n",
    "        tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=model.model_args.device)\n",
    "    \n",
    "    eos_reached = torch.tensor([False] * batch_size, device=model.model_args.device)\n",
    "    prompt_tokens_mask = tokens != pad_id # True if the token is a prompt token, False otherwise\n",
    "\n",
    "    return _beam1(model, tokens, prompt_tokens_mask, temperature, k) if func == 1 else _beam2(model, tokens, prompt_tokens_mask, temperature, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d6fe542d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot reshape tensor of 0 elements into shape [0, 1, 32, -1, 2] because the unspecified dimension size -1 can be any value and is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgenerate_with_beam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 227\u001b[39m, in \u001b[36mgenerate_with_beam\u001b[39m\u001b[34m(model, prompts, temperature, max_gen_len, k, func)\u001b[39m\n\u001b[32m    224\u001b[39m eos_reached = torch.tensor([\u001b[38;5;28;01mFalse\u001b[39;00m] * batch_size, device=model.model_args.device)\n\u001b[32m    225\u001b[39m prompt_tokens_mask = tokens != pad_id \u001b[38;5;66;03m# True if the token is a prompt token, False otherwise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_beam1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m func == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m _beam2(model, tokens, prompt_tokens_mask, temperature, k)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36m_beam1\u001b[39m\u001b[34m(model, tokens, prompt_tokens_mask, temperature, k)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cur_pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, L_total):\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# 1. Forward last token for every live beam --------------------------\u001b[39;00m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         logits = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens_flat\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_pos\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mcur_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_pos\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B·k, 1, V)\u001b[39;00m\n\u001b[32m     35\u001b[39m     log_probs = torch.log_softmax(logits[:, -\u001b[32m1\u001b[39m] / temperature, dim=-\u001b[32m1\u001b[39m)    \u001b[38;5;66;03m# (B·k, V)\u001b[39;00m\n\u001b[32m     36\u001b[39m     log_probs = log_probs.view(B, k, vocab_size)                          \u001b[38;5;66;03m# (B, k, V)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\anaconda3\\envs\\llama-2-from-scratch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\anaconda3\\envs\\llama-2-from-scratch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mz:\\Coding\\Projects\\llama-2 from scratch\\model.py:194\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, tokens, start_pos)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# sequentially apply all layers\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     h = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_complex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m h = \u001b[38;5;28mself\u001b[39m.norm(h)\n\u001b[32m    196\u001b[39m output = \u001b[38;5;28mself\u001b[39m.output(h).float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\anaconda3\\envs\\llama-2-from-scratch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\anaconda3\\envs\\llama-2-from-scratch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mz:\\Coding\\Projects\\llama-2 from scratch\\model.py:159\u001b[39m, in \u001b[36mEncoderBlock.forward\u001b[39m\u001b[34m(self, x, start_pos, freqs_complex)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor, start_pos: \u001b[38;5;28mint\u001b[39m, freqs_complex: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     h = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_complex\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, D) + (B, T, D) -> (B, T, D)\u001b[39;00m\n\u001b[32m    160\u001b[39m     out = h + \u001b[38;5;28mself\u001b[39m.feed_forward(\u001b[38;5;28mself\u001b[39m.ffn_norm(h))\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\anaconda3\\envs\\llama-2-from-scratch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\anaconda3\\envs\\llama-2-from-scratch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mz:\\Coding\\Projects\\llama-2 from scratch\\model.py:100\u001b[39m, in \u001b[36mSelfAttention.forward\u001b[39m\u001b[34m(self, x, start_pos, freqs_complex)\u001b[39m\n\u001b[32m     97\u001b[39m xk = xk.view(B, T, \u001b[38;5;28mself\u001b[39m.n_kv_heads, \u001b[38;5;28mself\u001b[39m.head_dim) \u001b[38;5;66;03m# (B, T, n_kv_heads * head_dim) -> (B, T, n_kv_heads, head_dim)\u001b[39;00m\n\u001b[32m     98\u001b[39m xv = xv.view(B, T, \u001b[38;5;28mself\u001b[39m.n_kv_heads, \u001b[38;5;28mself\u001b[39m.head_dim) \u001b[38;5;66;03m# (B, T, n_kv_heads * head_dim) -> (B, T, n_kv_heads, head_dim)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m xq = \u001b[43mapply_rotary_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_complex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, n_heads_q, head_dim) -> (B, T, n_heads_q, head_dim)\u001b[39;00m\n\u001b[32m    101\u001b[39m xk = apply_rotary_embeddings(xk, freqs_complex, device=x.device) \u001b[38;5;66;03m# (B, T, n_kv_heads, head_dim) -> (B, T, n_kv_heads, head_dim)\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# append to cache\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mz:\\Coding\\Projects\\llama-2 from scratch\\model.py:23\u001b[39m, in \u001b[36mapply_rotary_embeddings\u001b[39m\u001b[34m(x, freqs_complex, device)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_rotary_embeddings\u001b[39m(x: torch.Tensor, freqs_complex: torch.Tensor, device: \u001b[38;5;28mstr\u001b[39m) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     x_grouped = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, H, head_dim) -> (B, T, H, head_dim // 2, 2)\u001b[39;00m\n\u001b[32m     24\u001b[39m     x_grouped = x_grouped.float()\n\u001b[32m     25\u001b[39m     x_complex = torch.view_as_complex(x_grouped) \u001b[38;5;66;03m# (B, T, H, head_dim // 2, 2) -> (B, T, H, head_dim // 2)\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: cannot reshape tensor of 0 elements into shape [0, 1, 32, -1, 2] because the unspecified dimension size -1 can be any value and is ambiguous"
     ]
    }
   ],
   "source": [
    "generate_with_beam(model, prompts, max_gen_len=max_new_tokens, k=2, func=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8946a367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating tokens (beam search):   0%|          | 0/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot reshape tensor of 0 elements into shape [0, 1, 32, -1, 2] because the unspecified dimension size -1 can be any value and is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgenerate_with_beam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 227\u001b[39m, in \u001b[36mgenerate_with_beam\u001b[39m\u001b[34m(model, prompts, temperature, max_gen_len, k, func)\u001b[39m\n\u001b[32m    224\u001b[39m eos_reached = torch.tensor([\u001b[38;5;28;01mFalse\u001b[39;00m] * batch_size, device=model.model_args.device)\n\u001b[32m    225\u001b[39m prompt_tokens_mask = tokens != pad_id \u001b[38;5;66;03m# True if the token is a prompt token, False otherwise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _beam1(model, tokens, prompt_tokens_mask, temperature, k) \u001b[38;5;28;01mif\u001b[39;00m func == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_beam2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 129\u001b[39m, in \u001b[36m_beam2\u001b[39m\u001b[34m(model, tokens, prompt_tokens_mask, temperature, k)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cur_pos \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, total_len), desc=\u001b[33m\"\u001b[39m\u001b[33mGenerating tokens (beam search)\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# Get the logits for the next token\u001b[39;00m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# Input is the tokens up to cur_pos-1\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m         logits = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_pos\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mcur_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     \u001b[38;5;66;03m# Apply temperature and get log probabilities\u001b[39;00m\n\u001b[32m    132\u001b[39m     logits = logits[:, -\u001b[32m1\u001b[39m] / temperature\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\anaconda3\\envs\\llama-2-from-scratch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\anaconda3\\envs\\llama-2-from-scratch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mz:\\Coding\\Projects\\llama-2 from scratch\\model.py:194\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, tokens, start_pos)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# sequentially apply all layers\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     h = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_complex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m h = \u001b[38;5;28mself\u001b[39m.norm(h)\n\u001b[32m    196\u001b[39m output = \u001b[38;5;28mself\u001b[39m.output(h).float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\anaconda3\\envs\\llama-2-from-scratch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\anaconda3\\envs\\llama-2-from-scratch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mz:\\Coding\\Projects\\llama-2 from scratch\\model.py:159\u001b[39m, in \u001b[36mEncoderBlock.forward\u001b[39m\u001b[34m(self, x, start_pos, freqs_complex)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor, start_pos: \u001b[38;5;28mint\u001b[39m, freqs_complex: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     h = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_complex\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, D) + (B, T, D) -> (B, T, D)\u001b[39;00m\n\u001b[32m    160\u001b[39m     out = h + \u001b[38;5;28mself\u001b[39m.feed_forward(\u001b[38;5;28mself\u001b[39m.ffn_norm(h))\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\anaconda3\\envs\\llama-2-from-scratch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\parth\\anaconda3\\envs\\llama-2-from-scratch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mz:\\Coding\\Projects\\llama-2 from scratch\\model.py:100\u001b[39m, in \u001b[36mSelfAttention.forward\u001b[39m\u001b[34m(self, x, start_pos, freqs_complex)\u001b[39m\n\u001b[32m     97\u001b[39m xk = xk.view(B, T, \u001b[38;5;28mself\u001b[39m.n_kv_heads, \u001b[38;5;28mself\u001b[39m.head_dim) \u001b[38;5;66;03m# (B, T, n_kv_heads * head_dim) -> (B, T, n_kv_heads, head_dim)\u001b[39;00m\n\u001b[32m     98\u001b[39m xv = xv.view(B, T, \u001b[38;5;28mself\u001b[39m.n_kv_heads, \u001b[38;5;28mself\u001b[39m.head_dim) \u001b[38;5;66;03m# (B, T, n_kv_heads * head_dim) -> (B, T, n_kv_heads, head_dim)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m xq = \u001b[43mapply_rotary_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_complex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, n_heads_q, head_dim) -> (B, T, n_heads_q, head_dim)\u001b[39;00m\n\u001b[32m    101\u001b[39m xk = apply_rotary_embeddings(xk, freqs_complex, device=x.device) \u001b[38;5;66;03m# (B, T, n_kv_heads, head_dim) -> (B, T, n_kv_heads, head_dim)\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# append to cache\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mz:\\Coding\\Projects\\llama-2 from scratch\\model.py:23\u001b[39m, in \u001b[36mapply_rotary_embeddings\u001b[39m\u001b[34m(x, freqs_complex, device)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_rotary_embeddings\u001b[39m(x: torch.Tensor, freqs_complex: torch.Tensor, device: \u001b[38;5;28mstr\u001b[39m) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     x_grouped = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, H, head_dim) -> (B, T, H, head_dim // 2, 2)\u001b[39;00m\n\u001b[32m     24\u001b[39m     x_grouped = x_grouped.float()\n\u001b[32m     25\u001b[39m     x_complex = torch.view_as_complex(x_grouped) \u001b[38;5;66;03m# (B, T, H, head_dim // 2, 2) -> (B, T, H, head_dim // 2)\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: cannot reshape tensor of 0 elements into shape [0, 1, 32, -1, 2] because the unspecified dimension size -1 can be any value and is ambiguous"
     ]
    }
   ],
   "source": [
    "generate_with_beam(model, prompts, max_gen_len=max_new_tokens, k=2, func=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-2-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
